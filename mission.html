<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DEEPCALL | Mission</title>
    <link rel="stylesheet" href="css/style.css">
    <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@400;700&family=Inter:wght@300;400;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="grid-background"></div>
    
    <header>
        <div class="logo-container">
            <div class="logo">
                <div class="logo-icon">
                    <div class="node"></div>
                    <div class="node"></div>
                    <div class="node"></div>
                    <div class="curve"></div>
                </div>
                <div class="logo-text">DEEPCALL</div>
            </div>
        </div>
        
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="benchmark.html">Benchmarks</a></li>
                <li><a href="mission.html" class="active">Mission</a></li>
                <li><a href="contact.html">Contact</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <section class="mission-hero">
            <div class="glass-panel wide">
                <h1>Our <span class="highlight">Mission</span></h1>
                <p class="subtitle">Advancing AI for healthcare through rigorous evaluation and research with commitment to fairness and equity</p>
            </div>
        </section>

        <section class="mission-content">
            <div class="glass-panel">
                <div class="mission-grid">
                    <div class="mission-text">
                        <h2>Research <span class="highlight">Goals</span></h2>
                        <p>The goal of our research is to provide private evaluations that explore various knowledge domains and reasoning methods of small LLMs in the medical context while actively identifying and mitigating potential biases that could harm marginalized communities.</p>
                        
                        <p>We aim to understand how these models perform on clinical cases with varying levels of difficulty, starting with foundational knowledge and expanding to more complex scenarios. Our evaluation framework specifically tests for discriminatory patterns across gender, ethnicity, socioeconomic status, and other demographic factors.</p>
                        
                        <p>This work is part of a broader research effort to analyze reasoning patterns, knowledge representation, and domain-specific capabilities of language models, with potential applications in medical education and research tools development that serve all populations equitably.</p>
                        
                        <h3>Research Focus Areas:</h3>
                        <ul class="glow-list">
                            <li>Knowledge representation in medical domains with bias detection</li>
                            <li>Reasoning patterns across different model architectures and demographic groups</li>
                            <li>Performance on standardized clinical cases testing for fairness</li>
                            <li>Domain adaptation capabilities while preserving equity</li>
                        </ul>
                    </div>
                    
                    <div class="mission-image">
                        <img src="assets/deepcall3.png" alt="DEEPCALL AI Visualization" class="glow-image">
                    </div>
                </div>
                
                <div class="mission-section">
                    <h2>Our <span class="highlight">Approach</span></h2>
                    <p>We evaluate small portable LLMs (under 13B parameters) on relevant medicine daily practice concepts while rigorously testing for algorithmic bias and discrimination. Due to GDPR and HIPAA legislation, LOCAL small LLMs seem to be the appropriate answer to respect patients' privacy and patient-physician data use consent.</p>
                    
                    <p>Our evaluation uses a PRIVATE dataset of Questions/Answers in order to prevent data leakage and overfitting, with careful attention to ensuring diverse representation across all demographic groups. We put models "under stress" by testing them with clinical cases of potentially life-threatening situations while monitoring for biased responses that could perpetuate healthcare disparities.</p>
                    
                    <p>Our dataset of Question/Answer pairs intends to be the most relevant in terms of clinical benchmarking and includes scenarios designed to detect discriminatory patterns against women, ethnic minorities, LGBTQ+ individuals, and other underrepresented populations. All answers are graded manually by Doctor Yohann Missiak MD, specialist of Chemical Pathology, clinical laboratory Director and Professor of Master CEMP AI Healthcare, Chichester.</p>
                </div>
                
                <div class="mission-section">
                    <h2>Important <span class="highlight">Limitations</span></h2>
                    <ul class="limitation-list">
                        <li>This is an experimental research project, not a clinical validation, and bias detection methods are still evolving</li>
                        <li>Performance on this test does not indicate clinical competence or relevancy, nor does it guarantee absence of discriminatory behavior</li>
                        <li>No LLM should be used for actual medical advice, diagnostics or decision-making, especially given potential for hidden biases</li>
                        <li>This evaluation was not peer-reviewed or validated for clinical use, and fairness assessments require ongoing refinement</li>
                        <li>Results are shared for research purposes only to advance understanding of AI bias in healthcare</li>
                    </ul>
                    
                    <div class="disclaimer-box">
                        <h3>Medical Disclaimer</h3>
                        <p>This test evaluation is for experimental research purposes only. The author Dr. Yohann Missiak makes no claims about the medical reliability of any of these models and expressly disclaims any liability for their use in any medical context. Models may exhibit biases that could lead to discriminatory outcomes in healthcare settings.</p>
                    </div>
                </div>
                
                <div class="mission-section">
                    <h2>Future <span class="highlight">Directions</span></h2>
                    <p>We are expanding our evaluation to include more medical specialties and creating a comprehensive leaderboard of model performance that includes fairness metrics alongside accuracy measures. Future versions will explore more complex clinical reasoning scenarios beyond the foundational knowledge currently tested, with enhanced focus on detecting and mitigating biases across diverse patient populations.</p>
                    
                    <p>Our goal is to contribute to the responsible development of AI in healthcare by providing rigorous, transparent evaluations that highlight both capabilities and limitations of current models while advancing the field's understanding of how to build more equitable and inclusive medical AI systems.</p>
                </div>
            </div>
        </section>
    </main>

    <footer>
        <div class="footer-content">
            <div class="footer-logo">DEEPCALL</div>
            <div class="footer-links">
                <a href="index.html">Home</a>
                <a href="benchmark.html">Benchmark</a>
                <a href="mission.html">Mission</a>
                <a href="contact.html">Contact</a>
            </div>
            <div class="footer-legal">
                <p>Â© 2025 DEEPCALL Technology | Dr. Yohann Missiak</p>
                <p>Research purposes only. Not for clinical use.</p>
            </div>
        </div>
    </footer>
</body>
</html>
